// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeDefinition 1`] = `
{
  "bases": [
    {
      "module": [
        "vellum",
        "workflows",
        "nodes",
        "displayable",
      ],
      "name": "InlinePromptNode",
    },
  ],
  "module": [
    "code",
    "nodes",
    "prompt_node",
  ],
  "name": "PromptNode",
}
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > basic > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import (
    PlainTextPromptBlock,
    VariablePromptBlock,
    RichTextPromptBlock,
    ChatMessagePromptBlock,
    PromptParameters,
)
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            state="ENABLED",
            cache_config=None,
            chat_role="SYSTEM",
            chat_source=None,
            chat_message_unterminated=False,
            blocks=[
                RichTextPromptBlock(
                    state="ENABLED",
                    cache_config=None,
                    blocks=[
                        PlainTextPromptBlock(
                            state="ENABLED",
                            cache_config=None,
                            text="Summarize the following text:\\n\\n",
                        ),
                        VariablePromptBlock(
                            state="ENABLED", cache_config=None, input_variable="text"
                        ),
                    ],
                )
            ],
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > legacy prompt variant > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > legacy prompt variant > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import (
    PlainTextPromptBlock,
    VariablePromptBlock,
    RichTextPromptBlock,
    ChatMessagePromptBlock,
    PromptParameters,
)
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            state="ENABLED",
            cache_config=None,
            chat_role="SYSTEM",
            chat_source=None,
            chat_message_unterminated=False,
            blocks=[
                RichTextPromptBlock(
                    state="ENABLED",
                    cache_config=None,
                    blocks=[
                        PlainTextPromptBlock(
                            state="ENABLED",
                            cache_config=None,
                            text="Summarize the following text:\\n\\n",
                        ),
                        VariablePromptBlock(
                            state="ENABLED", cache_config=None, input_variable="text"
                        ),
                    ],
                )
            ],
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > reject on error enabled > getNodeDisplayFile for CHAT_MESSAGE block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseTryNodeDisplay,
    BaseInlinePromptNodeDisplay,
)
from uuid import UUID
from ...nodes.prompt_node import PromptNode
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class TryNodeDisplay(BaseTryNodeDisplay):
    error_output_id = UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f")


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > CHAT_MESSAGE block type > reject on error enabled > getNodeFile for CHAT_MESSAGE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import (
    PlainTextPromptBlock,
    VariablePromptBlock,
    RichTextPromptBlock,
    ChatMessagePromptBlock,
    PromptParameters,
)
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        ChatMessagePromptBlock(
            state="ENABLED",
            cache_config=None,
            chat_role="SYSTEM",
            chat_source=None,
            chat_message_unterminated=False,
            blocks=[
                RichTextPromptBlock(
                    state="ENABLED",
                    cache_config=None,
                    blocks=[
                        PlainTextPromptBlock(
                            state="ENABLED",
                            cache_config=None,
                            text="Summarize the following text:\\n\\n",
                        ),
                        VariablePromptBlock(
                            state="ENABLED", cache_config=None, input_variable="text"
                        ),
                    ],
                )
            ],
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeDefinition 1`] = `
{
  "bases": [
    {
      "module": [
        "vellum",
        "workflows",
        "nodes",
        "displayable",
      ],
      "name": "InlinePromptNode",
    },
  ],
  "module": [
    "code",
    "nodes",
    "prompt_node",
  ],
  "name": "PromptNode",
}
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > basic > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import PromptParameters, FunctionDefinition
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function")
    ]
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > legacy prompt variant > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > legacy prompt variant > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import PromptParameters, FunctionDefinition
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function")
    ]
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > reject on error enabled > getNodeDisplayFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseTryNodeDisplay,
    BaseInlinePromptNodeDisplay,
)
from uuid import UUID
from ...nodes.prompt_node import PromptNode
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class TryNodeDisplay(BaseTryNodeDisplay):
    error_output_id = UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f")


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > FUNCTION_DEFINITION block type > reject on error enabled > getNodeFile for FUNCTION_DEFINITION block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import PromptParameters, FunctionDefinition
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = []
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
    functions = [
        FunctionDefinition(name="functionTest", description="This is a test function")
    ]
"
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeDefinition 1`] = `
{
  "bases": [
    {
      "module": [
        "vellum",
        "workflows",
        "nodes",
        "displayable",
      ],
      "name": "InlinePromptNode",
    },
  ],
  "module": [
    "code",
    "nodes",
    "prompt_node",
  ],
  "name": "PromptNode",
}
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeDisplayFile for JINJA block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > JINJA block type > basic > getNodeFile for JINJA block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(
            state="ENABLED",
            cache_config=None,
            template="Summarize what this means {{ INPUT_VARIABLE }}",
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > JINJA block type > legacy prompt variant > getNodeDisplayFile for JINJA block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > JINJA block type > legacy prompt variant > getNodeFile for JINJA block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(
            state="ENABLED",
            cache_config=None,
            template="Summarize what this means {{ INPUT_VARIABLE }}",
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > JINJA block type > reject on error enabled > getNodeDisplayFile for JINJA block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseTryNodeDisplay,
    BaseInlinePromptNodeDisplay,
)
from uuid import UUID
from ...nodes.prompt_node import PromptNode
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class TryNodeDisplay(BaseTryNodeDisplay):
    error_output_id = UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f")


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > JINJA block type > reject on error enabled > getNodeFile for JINJA block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import JinjaPromptBlock, PromptParameters
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        JinjaPromptBlock(
            state="ENABLED",
            cache_config=None,
            template="Summarize what this means {{ INPUT_VARIABLE }}",
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeDefinition 1`] = `
{
  "bases": [
    {
      "module": [
        "vellum",
        "workflows",
        "nodes",
        "displayable",
      ],
      "name": "InlinePromptNode",
    },
  ],
  "module": [
    "code",
    "nodes",
    "prompt_node",
  ],
  "name": "PromptNode",
}
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > basic > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import PlainTextPromptBlock, RichTextPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(
            state="ENABLED",
            cache_config=None,
            blocks=[
                PlainTextPromptBlock(
                    state="ENABLED", cache_config=None, text="Hello World!"
                )
            ],
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > legacy prompt variant > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > legacy prompt variant > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import PlainTextPromptBlock, RichTextPromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(
            state="ENABLED",
            cache_config=None,
            blocks=[
                PlainTextPromptBlock(
                    state="ENABLED", cache_config=None, text="Hello World!"
                )
            ],
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > reject on error enabled > getNodeDisplayFile for RICH_TEXT block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseTryNodeDisplay,
    BaseInlinePromptNodeDisplay,
)
from uuid import UUID
from ...nodes.prompt_node import PromptNode
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class TryNodeDisplay(BaseTryNodeDisplay):
    error_output_id = UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f")


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > RICH_TEXT block type > reject on error enabled > getNodeFile for RICH_TEXT block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import PlainTextPromptBlock, RichTextPromptBlock, PromptParameters
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        RichTextPromptBlock(
            state="ENABLED",
            cache_config=None,
            blocks=[
                PlainTextPromptBlock(
                    state="ENABLED", cache_config=None, text="Hello World!"
                )
            ],
        )
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeDefinition 1`] = `
{
  "bases": [
    {
      "module": [
        "vellum",
        "workflows",
        "nodes",
        "displayable",
      ],
      "name": "InlinePromptNode",
    },
  ],
  "module": [
    "code",
    "nodes",
    "prompt_node",
  ],
  "name": "PromptNode",
}
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeDisplayFile for VARIABLE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > basic > getNodeFile for VARIABLE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import VariablePromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(state="ENABLED", cache_config=None, input_variable="text")
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > VARIABLE block type > legacy prompt variant > getNodeDisplayFile for VARIABLE block type 1`] = `
"from vellum_ee.workflows.display.nodes import BaseInlinePromptNodeDisplay
from ...nodes.prompt_node import PromptNode
from uuid import UUID
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > legacy prompt variant > getNodeFile for VARIABLE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum import VariablePromptBlock, PromptParameters
from ..inputs import Inputs


class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(state="ENABLED", cache_config=None, input_variable="text")
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;

exports[`InlinePromptNode > VARIABLE block type > reject on error enabled > getNodeDisplayFile for VARIABLE block type 1`] = `
"from vellum_ee.workflows.display.nodes import (
    BaseTryNodeDisplay,
    BaseInlinePromptNodeDisplay,
)
from uuid import UUID
from ...nodes.prompt_node import PromptNode
from vellum_ee.workflows.display.nodes.types import (
    NodeOutputDisplay,
    PortDisplayOverrides,
)
from vellum_ee.workflows.display.vellum import NodeDisplayData, NodeDisplayPosition


class TryNodeDisplay(BaseTryNodeDisplay):
    error_output_id = UUID("e7a1fbea-f5a7-4b31-a9ff-0d26c3de021f")


class PromptNodeDisplay(BaseInlinePromptNodeDisplay[PromptNode]):
    label = "Prompt Node"
    node_id = UUID("7e09927b-6d6f-4829-92c9-54e66bdcaf80")
    output_id = UUID("2d4f1826-de75-499a-8f84-0a690c8136ad")
    array_output_id = UUID("771c6fba-5b4a-4092-9d52-693242d7b92c")
    target_handle_id = UUID("3feb7e71-ec63-4d58-82ba-c3df829a2948")
    prompt_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    node_input_ids_by_name = {"text": UUID("7b8af68b-cf60-4fca-9c57-868042b5b616")}
    output_display = {
        PromptNode.Outputs.text: NodeOutputDisplay(
            id=UUID("2d4f1826-de75-499a-8f84-0a690c8136ad"), name="text"
        ),
        PromptNode.Outputs.results: NodeOutputDisplay(
            id=UUID("771c6fba-5b4a-4092-9d52-693242d7b92c"), name="results"
        ),
    }
    port_displays = {
        PromptNode.Ports.default: PortDisplayOverrides(
            id=UUID("dd8397b1-5a41-4fa0-8c24-e5dffee4fb98")
        )
    }
    display_data = NodeDisplayData(
        position=NodeDisplayPosition(x=0, y=0), width=None, height=None
    )
"
`;

exports[`InlinePromptNode > VARIABLE block type > reject on error enabled > getNodeFile for VARIABLE block type 1`] = `
"from vellum.workflows.nodes.displayable import InlinePromptNode
from vellum.workflows.nodes.core import TryNode
from vellum import VariablePromptBlock, PromptParameters
from ..inputs import Inputs


@TryNode.wrap()
class PromptNode(InlinePromptNode):
    ml_model = "gpt-4o-mini"
    blocks = [
        VariablePromptBlock(state="ENABLED", cache_config=None, input_variable="text")
    ]
    parameters = PromptParameters(
        stop=None,
        temperature=0,
        max_tokens=1000,
        top_p=1,
        top_k=0,
        frequency_penalty=0,
        presence_penalty=0,
        logit_bias={},
        custom_parameters={},
    )
    prompt_inputs = {"text": Inputs.text_1}
"
`;
